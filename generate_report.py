#!/usr/bin/env python3
"""
Generate a standalone LaTeX report from agent logs and baseline results.
Reads all data from results/ and test_data/ directories.
"""

import json
import csv
import re
from pathlib import Path
from datetime import datetime
from collections import defaultdict

PROJECT_DIR = Path(__file__).parent
RESULTS_DIR = PROJECT_DIR / "results"
TEST_DATA_DIR = PROJECT_DIR / "test_data"


def escape_latex(text: str) -> str:
    """Escape special LaTeX characters."""
    replacements = {
        '&': r'\&',
        '%': r'\%',
        '$': r'\$',
        '#': r'\#',
        '_': r'\_',
        '{': r'\{',
        '}': r'\}',
        '~': r'\textasciitilde{}',
        '^': r'\textasciicircum{}',
    }
    for old, new in replacements.items():
        text = text.replace(old, new)
    return text


def load_ground_truth(dataset_name: str):
    gt_file = TEST_DATA_DIR / dataset_name / "ground_truth.json"
    with open(gt_file) as f:
        data = json.load(f)
    return data


def load_baseline(dataset_name: str):
    csv_file = RESULTS_DIR / "baseline" / f"{dataset_name}_predictions.csv"
    if not csv_file.exists():
        return []
    with open(csv_file) as f:
        return list(csv.DictReader(f))


def load_agent_logs(dataset_name: str):
    logs_dir = RESULTS_DIR / "agent" / "logs" / dataset_name
    if not logs_dir.exists():
        return []
    logs = []
    for log_file in sorted(logs_dir.glob("test_*_log.json")):
        with open(log_file) as f:
            logs.append(json.load(f))
    return logs


def compute_metrics(results, expected_classes):
    """Compute overall and per-class accuracy from a list of dicts with 'ground_truth', 'prediction', 'correct'."""
    total = len(results)
    correct = sum(1 for r in results if r.get("correct") in (True, "True"))
    accuracy = (correct / total * 100) if total > 0 else 0.0

    class_stats = defaultdict(lambda: {"correct": 0, "total": 0})
    for r in results:
        gt = r["ground_truth"]
        class_stats[gt]["total"] += 1
        if r.get("correct") in (True, "True"):
            class_stats[gt]["correct"] += 1

    per_class = {}
    for cls in expected_classes:
        s = class_stats.get(cls, {"correct": 0, "total": 0})
        per_class[cls] = {
            "correct": s["correct"],
            "total": s["total"],
            "accuracy": (s["correct"] / s["total"] * 100) if s["total"] > 0 else 0.0
        }

    return {"total": total, "correct": correct, "accuracy": accuracy, "per_class": per_class}


def format_trace_for_latex(trace):
    """Format the agent trace as a LaTeX itemized list."""
    lines = []
    for i, step in enumerate(trace):
        if step["type"] == "tool_use":
            tool = step.get("tool", "?")
            fp = step.get("input", {}).get("file_path", "")
            short_path = fp.split("reasoning/")[-1] if "reasoning/" in fp else fp
            lines.append(f"\\item[\\textbf{{Tool:}}] \\texttt{{{escape_latex(tool)}}} $\\rightarrow$ \\texttt{{{escape_latex(short_path)}}}")
        elif step["type"] == "text":
            content = step.get("content", "")
            lines.append(f"\\item[\\textbf{{Think:}}] {escape_latex(content)}")
    return "\n".join(lines)


def format_reasoning_for_latex(reasoning):
    """Format final reasoning text for LaTeX."""
    return escape_latex(reasoning)


def generate_latex():
    datasets = ["Foliar_Disease_Stress"]

    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # --- Build LaTeX document ---
    doc = []
    doc.append(r"""\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{amssymb}

\definecolor{correct}{RGB}{34,139,34}
\definecolor{wrong}{RGB}{200,30,30}
\definecolor{neutral}{RGB}{80,80,80}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Plant Disease Classification Report}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

\title{Plant Disease Classification:\\Baseline vs Agent Evaluation Report}
\author{Generated by \texttt{generate\_report.py}}
""")
    doc.append(f"\\date{{{escape_latex(now)}}}")
    doc.append(r"""
\begin{document}
\maketitle
\tableofcontents
\newpage
""")

    # ============================
    # Section 1: Executive Summary
    # ============================
    doc.append(r"""
\section{Executive Summary}

This report compares two approaches for plant disease image classification using Claude:
\begin{itemize}
  \item \textbf{Baseline}: Direct API call to Claude Haiku with only the image and class list (single inference, no context).
  \item \textbf{Agent}: Claude Code headless mode (\texttt{claude -p}) where the agent reads a symptom knowledge base, views the test image, views reference images, reasons through candidates, and then predicts.
\end{itemize}
""")

    # Overall summary table
    doc.append(r"""
\subsection{Overall Results}
\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Dataset} & \textbf{Baseline} & \textbf{Agent} & \textbf{Improvement} \\
\midrule
""")

    all_data = {}
    for ds in datasets:
        gt_data = load_ground_truth(ds)
        expected_classes = gt_data["expected_classes"]
        baseline = load_baseline(ds)
        agent_logs = load_agent_logs(ds)

        b_metrics = compute_metrics(baseline, expected_classes)
        a_metrics = compute_metrics(agent_logs, expected_classes)
        diff = a_metrics["accuracy"] - b_metrics["accuracy"]
        diff_str = f"+{diff:.1f}\\%" if diff > 0 else f"{diff:.1f}\\%"

        color = "correct" if diff > 0 else ("wrong" if diff < 0 else "neutral")

        doc.append(f"{escape_latex(ds)} & {b_metrics['accuracy']:.1f}\\% & {a_metrics['accuracy']:.1f}\\% & \\textcolor{{{color}}}{{{diff_str}}} \\\\")

        all_data[ds] = {
            "gt_data": gt_data,
            "expected_classes": expected_classes,
            "baseline": baseline,
            "agent_logs": agent_logs,
            "b_metrics": b_metrics,
            "a_metrics": a_metrics,
        }

    doc.append(r"""
\bottomrule
\end{tabular}
\caption{Overall accuracy comparison across datasets.}
\end{table}
""")

    # Agent cost/time summary
    doc.append(r"""
\subsection{Agent Cost and Latency}
\begin{table}[h]
\centering
\begin{tabular}{lrrrrr}
\toprule
\textbf{Dataset} & \textbf{Images} & \textbf{Avg Turns} & \textbf{Avg Duration (s)} & \textbf{Avg Cost (\$)} & \textbf{Total Cost (\$)} \\
\midrule
""")
    for ds, data in all_data.items():
        logs = data["agent_logs"]
        n = len(logs)
        if n > 0:
            avg_turns = sum(l.get("num_turns", 0) or 0 for l in logs) / n
            avg_dur = sum(l.get("duration_ms", 0) or 0 for l in logs) / n / 1000
            avg_cost = sum(l.get("cost_usd", 0) or 0 for l in logs) / n
            total_cost = sum(l.get("cost_usd", 0) or 0 for l in logs)
            doc.append(f"{escape_latex(ds)} & {n} & {avg_turns:.1f} & {avg_dur:.1f} & {avg_cost:.4f} & {total_cost:.4f} \\\\")

    doc.append(r"""
\bottomrule
\end{tabular}
\caption{Agent execution statistics.}
\end{table}
\newpage
""")

    # ============================
    # Section 2+: Per-dataset details
    # ============================
    for ds, data in all_data.items():
        gt_data = data["gt_data"]
        expected_classes = data["expected_classes"]
        baseline = data["baseline"]
        agent_logs = data["agent_logs"]
        b_metrics = data["b_metrics"]
        a_metrics = data["a_metrics"]

        desc = gt_data.get("description", ds)
        doc.append(f"\\section{{{escape_latex(ds)}: {escape_latex(desc)}}}")

        # Per-class table
        doc.append(r"""
\subsection{Per-Class Accuracy}
\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Class} & \textbf{Baseline} & \textbf{Agent} & \textbf{Diff} \\
\midrule
""")
        for cls in expected_classes:
            b_cls = b_metrics["per_class"].get(cls, {"correct": 0, "total": 0, "accuracy": 0})
            a_cls = a_metrics["per_class"].get(cls, {"correct": 0, "total": 0, "accuracy": 0})
            cls_diff = a_cls["accuracy"] - b_cls["accuracy"]

            b_str = f"{b_cls['correct']}/{b_cls['total']}"
            a_str = f"{a_cls['correct']}/{a_cls['total']}"
            if cls_diff > 0:
                diff_str = f"\\textcolor{{correct}}{{+{cls_diff:.0f}\\%}}"
            elif cls_diff < 0:
                diff_str = f"\\textcolor{{wrong}}{{{cls_diff:.0f}\\%}}"
            else:
                diff_str = "0\\%"

            doc.append(f"{escape_latex(cls)} & {b_str} & {a_str} & {diff_str} \\\\")

        doc.append(r"""
\bottomrule
\end{tabular}
\caption{Per-class accuracy for """ + escape_latex(ds) + r""".}
\end{table}
""")

        # Image-by-image comparison table
        baseline_map = {r["image_name"]: r for r in baseline}
        agent_map = {l["image_name"]: l for l in agent_logs}

        doc.append(r"""
\subsection{Image-by-Image Results}
\begin{longtable}{llllcllc}
\toprule
\textbf{\#} & \textbf{Image} & \textbf{Ground Truth} & \textbf{Baseline} & & \textbf{Agent} & & \textbf{Turns} \\
\midrule
\endhead
""")
        all_images = sorted(set(list(baseline_map.keys()) + list(agent_map.keys())))
        for i, img in enumerate(all_images, 1):
            gt_label = baseline_map.get(img, agent_map.get(img, {})).get("ground_truth", "?")
            b_pred = baseline_map.get(img, {}).get("prediction", "—")
            b_ok = baseline_map.get(img, {}).get("correct") in (True, "True")
            a_pred = agent_map.get(img, {}).get("prediction", "—")
            a_ok = agent_map.get(img, {}).get("correct") in (True, "True")
            turns = agent_map.get(img, {}).get("num_turns", "—")

            b_mark = r"\textcolor{correct}{$\checkmark$}" if b_ok else r"\textcolor{wrong}{$\times$}"
            a_mark = r"\textcolor{correct}{$\checkmark$}" if a_ok else r"\textcolor{wrong}{$\times$}"

            doc.append(f"{i} & \\texttt{{{escape_latex(img)}}} & {escape_latex(gt_label)} & {escape_latex(b_pred)} & {b_mark} & {escape_latex(a_pred)} & {a_mark} & {turns} \\\\")

        doc.append(r"""
\bottomrule
\end{longtable}
""")

        # ============================
        # Reasoning Traces
        # ============================
        doc.append(r"\subsection{Agent Reasoning Traces}")

        # Separate correct and incorrect
        correct_logs = [l for l in agent_logs if l.get("correct") is True]
        incorrect_logs = [l for l in agent_logs if l.get("correct") is not True]

        # --- Correct predictions ---
        if correct_logs:
            doc.append(r"""
\subsubsection{Correct Predictions}
""")
            for log in correct_logs:
                img = log["image_name"]
                gt = log["ground_truth"]
                pred = log["prediction"]
                turns = log.get("num_turns", "?")
                cost = log.get("cost_usd", 0) or 0
                dur = (log.get("duration_ms", 0) or 0) / 1000

                doc.append(f"\\paragraph{{{escape_latex(img)} — \\textcolor{{correct}}{{Correct}}}}")
                doc.append(f"Ground Truth: \\textbf{{{escape_latex(gt)}}} $\\rightarrow$ Prediction: \\textbf{{{escape_latex(pred)}}} \\\\")
                doc.append(f"Turns: {turns} \\quad Duration: {dur:.1f}s \\quad Cost: \\${cost:.4f}")

                # Trace
                trace = log.get("trace", [])
                if trace:
                    doc.append(r"\begin{description}[style=unboxed,leftmargin=0.5cm]")
                    doc.append(format_trace_for_latex(trace))
                    doc.append(r"\end{description}")

                doc.append(r"\medskip\hrule\medskip")

        # --- Incorrect predictions ---
        if incorrect_logs:
            doc.append(r"""
\subsubsection{Incorrect Predictions --- Error Analysis}
""")
            for log in incorrect_logs:
                img = log["image_name"]
                gt = log["ground_truth"]
                pred = log["prediction"]
                turns = log.get("num_turns", "?")
                cost = log.get("cost_usd", 0) or 0
                dur = (log.get("duration_ms", 0) or 0) / 1000

                doc.append(f"\\paragraph{{{escape_latex(img)} — \\textcolor{{wrong}}{{Incorrect}}}}")
                doc.append(f"Ground Truth: \\textbf{{{escape_latex(gt)}}} $\\rightarrow$ Prediction: \\textbf{{\\textcolor{{wrong}}{{{escape_latex(pred)}}}}} \\\\")
                doc.append(f"Turns: {turns} \\quad Duration: {dur:.1f}s \\quad Cost: \\${cost:.4f}")

                # Trace
                trace = log.get("trace", [])
                if trace:
                    doc.append(r"\begin{description}[style=unboxed,leftmargin=0.5cm]")
                    doc.append(format_trace_for_latex(trace))
                    doc.append(r"\end{description}")

                # Final reasoning
                reasoning = log.get("reasoning", "")
                if reasoning:
                    doc.append(r"\textbf{Final Reasoning:}")
                    doc.append(r"\begin{quote}")
                    doc.append(r"\small\itshape " + format_reasoning_for_latex(reasoning))
                    doc.append(r"\end{quote}")

                doc.append(r"\medskip\hrule\medskip")

        doc.append(r"\newpage")

    # ============================
    # End document
    # ============================
    doc.append(r"\end{document}")

    # Write LaTeX file
    latex_content = "\n".join(doc)
    output_file = RESULTS_DIR / "evaluation_report.tex"
    with open(output_file, "w") as f:
        f.write(latex_content)

    print(f"LaTeX report generated: {output_file}")
    print(f"Compile with: pdflatex {output_file}")
    return output_file


if __name__ == "__main__":
    generate_latex()
